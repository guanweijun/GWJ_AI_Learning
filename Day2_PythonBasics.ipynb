{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanweijun/GWJ_AI_Learning/blob/main/Day2_PythonBasics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np   #torch 就是 PyTorch 的顶级模块名，多维数组类 torch.Tensor（在 GPU/CPU 上都能跑）\n",
        "\n",
        "np_arr = np.arange(6).reshape(2,3)  #arrange（）是Numpy数组\n",
        "tensor = torch.from_numpy(np_arr) #把NumPy数组np_arr零拷贝地变成 PyTorch 张量，二者共享同一块内存——改 NumPy 值就等于改 Tensor 值，反之亦然\n",
        "\n",
        "print(\"NumPy:\\n\", np_arr)\n",
        "print(\"Tensor:\\n\", tensor)\n",
        "print(\"Tensor 形状:\", tensor.shape)\n",
        "print(\"元素个数:\", tensor.numel())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYYw1_KfI2IB",
        "outputId": "d5c4e369-d936-4e15-be78-6fbec1ccce60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy:\n",
            " [[0 1 2]\n",
            " [3 4 5]]\n",
            "Tensor:\n",
            " tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "Tensor 形状: torch.Size([2, 3])\n",
            "元素个数: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "back = tensor.numpy() #把 PyTorch 张量再“变回” NumPy 数组，虽然数据是同一块内存，但 Python 会新建一个“数组外壳”对象。\n",
        "assert np_arr is back #is 比较的是“是不是同一个 Python 对象”（内存地址相同才返回 True）。因为外壳是新的，所以 np_arr is back 得到 False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "UoFghk9IL7kS",
        "outputId": "28fb880c-e544-4a95-e59a-5ea904e5ad38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2567311102.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#把 PyTorch 张量再“变回” NumPy 数组，虽然数据是同一块内存，但 Python 会新建一个“数组外壳”对象。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp_arr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mback\u001b[0m \u001b[0;31m#is 比较的是“是不是同一个 Python 对象”（内存地址相同才返回 True）。因为外壳是新的，所以 np_arr is back 得到 False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#用tensor来计算梯度，因为只有这样才能调用出计算图，才能调 backward() 得到梯度\n",
        "x = torch.tensor(3.0, requires_grad=False)   #把输入特征随便设成 3.0，当作已知常量。还是原来tensor格式\n",
        "print(\"x:\\n\", x)\n",
        "w = torch.tensor(2.0, requires_grad=True)  #把可训练权重随便设成 2.0，当作初始参数，后面靠梯度下降去更新。还是原来tensor格式\n",
        "print(\"w:\\n\", w)\n",
        "y_true = torch.tensor(9.0) #没有特殊含义，就是告诉 PyTorch：“把这枚小数 9.0 包成张量，让我后面能直接跟其它张量做运算。”\n",
        "\n",
        "loss = (w * x - y_true)**2      # 手工 MSE，高维度应该用(w * x - y).pow(2).mean()\n",
        "loss.backward()     #它把计算图里所有需要梯度的叶子节点（requires_grad=True的，本例只有 w）的梯度一次性算出来，并存到对应的 .grad 属性里。\n",
        "print(\"∂loss/∂w =\", w.grad.item())   # 期望 2*(w*x-y_true)*x = 2*(6-9)*3 = -18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbAG4MT1MtVC",
        "outputId": "1cf4be3d-2dbd-42c9-aca3-d68e548a01eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            " tensor(3.)\n",
            "w:\n",
            " tensor(2., requires_grad=True)\n",
            "∂loss/∂w = -18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#线性回归 CPU\n",
        "#造数据\n",
        "# 100 个点  y = 5x + 4  + 噪声\n",
        "X = torch.linspace(0, 10, 100).reshape(-1,1) #在 0 到 10 之间均匀取出 100 个数，先排成 1 维，再改成 100×1 的列向量。-1指不制定行数，自己算\n",
        "true_w, true_b = 5.0, 4.0\n",
        "y = true_w * X + true_b + torch.randn(X.size()) #生成一个和 X 同样形状的随机张量\n",
        "\n",
        "#定义参数\n",
        "w = torch.randn(1, requires_grad=True) #生成 1 个服从标准正态分布 N(0,1) 的随机数\n",
        "b = torch.randn(1, requires_grad=True) #生成 1 个服从标准正态分布 N(0,1) 的随机数\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#训练循环（显性写法，便于看懂）\n",
        "lr = 2.7e-2     # 0.001~0.1 是多年实验的“舒适区”，学习率 0.5 太大，一步迈过“谷底”，每次都在对岸来回蹦，grad越蹦越远。0.1 先跑，稳就留；乱跳减半（0.01），慢就加半（0.3）\n",
        "for epoch in range(500):\n",
        "    y_pred = w * X + b\n",
        "    loss = ((y_pred - y)**2).mean()\n",
        "\n",
        "    loss.backward()   #计算每次循环的瞬时梯度\n",
        "    if epoch % 50 == 0:\n",
        "      print(f\"epoch {epoch:3d} w.grad={w.grad.item():.3f}  b.grad={b.grad.item():.3f} \")\n",
        "    with torch.no_grad():        # 手动更新， 是临时关闭梯度计算的上下文管理器，不会被 autograd 记录到计算图里，因此 .grad 不会被更新。\n",
        "                      #一句话：更新参数时必须脱离计算图，否则图会无限膨胀且梯度完全不对\n",
        "        w -= lr * w.grad\n",
        "        b -= lr * b.grad\n",
        "        w.grad.zero_(); b.grad.zero_()  #不清零，会和上一轮的梯度累加\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"epoch {epoch:3d}  loss={loss:.4f}  w={w.item():.3f}  b={b.item():.3f}\")\n",
        "\n",
        "print(\"CPU 耗时:\", time.time() - start, \"s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhKI2qhbZZCc",
        "outputId": "61f1a862-71ff-4c05-8cff-07308e52efe9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   0 w.grad=-366.685  b.grad=-56.593 \n",
            "epoch   0  loss=1007.8395  w=9.974  b=1.920\n",
            "epoch  50 w.grad=0.003  b.grad=-0.742 \n",
            "epoch  50  loss=1.5811  w=5.215  b=2.626\n",
            "epoch 100 w.grad=0.056  b.grad=-0.370 \n",
            "epoch 100  loss=1.1786  w=5.107  b=3.334\n",
            "epoch 150 w.grad=0.028  b.grad=-0.188 \n",
            "epoch 150  loss=1.0743  w=5.053  b=3.695\n",
            "epoch 200 w.grad=0.014  b.grad=-0.096 \n",
            "epoch 200  loss=1.0472  w=5.025  b=3.879\n",
            "epoch 250 w.grad=0.007  b.grad=-0.049 \n",
            "epoch 250  loss=1.0402  w=5.011  b=3.972\n",
            "epoch 300 w.grad=0.004  b.grad=-0.025 \n",
            "epoch 300  loss=1.0383  w=5.004  b=4.020\n",
            "epoch 350 w.grad=0.002  b.grad=-0.013 \n",
            "epoch 350  loss=1.0379  w=5.001  b=4.044\n",
            "epoch 400 w.grad=0.001  b.grad=-0.006 \n",
            "epoch 400  loss=1.0377  w=4.999  b=4.057\n",
            "epoch 450 w.grad=0.001  b.grad=-0.003 \n",
            "epoch 450  loss=1.0377  w=4.998  b=4.063\n",
            "CPU 耗时: 0.12009215354919434 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#线性回归，CUDA\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using\", device)\n",
        "\n",
        "X_gpu = X.to(device); y_gpu = y.to(device)\n",
        "w_gpu = torch.randn(1, requires_grad=True, device=device)\n",
        "b_gpu = torch.randn(1, device=device)\n",
        "b_gpu.requires_grad_(True)    #原地打开梯度开关\n",
        "\n",
        "# 同样循环 500 epoch，计时\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "#训练循环\n",
        "lr = 2.7e-2\n",
        "for epoch in range(501):\n",
        "    y_pred = w_gpu * X_gpu + b_gpu\n",
        "    loss = ((y_pred - y_gpu)**2).mean()\n",
        "\n",
        "    loss.backward()\n",
        "    if epoch % 50 == 0:\n",
        "      print(f\"epoch {epoch:3d} w_gpu.grad={w_gpu.grad.item():.3f}  b_gpu.grad={b_gpu.grad.item():.3f} \")\n",
        "    with torch.no_grad():\n",
        "        w_gpu -= lr * w_gpu.grad\n",
        "        b_gpu -= lr * b_gpu.grad\n",
        "        w_gpu.grad.zero_(); b_gpu.grad.zero_()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"epoch {epoch:3d}  loss={loss:.4f}  w_gpu={w_gpu.item():.3f}  b_gpu={b_gpu.item():.3f}\")\n",
        "\n",
        "\n",
        "print(\"GPU 耗时:\", time.time() - start, \"s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9LLC_eFl_Cl",
        "outputId": "583a40dc-aeaa-4b69-f470-fd8ba2b60bd9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n",
            "epoch   0 w_gpu.grad=-311.562  b_gpu.grad=-47.725 \n",
            "epoch   0  loss=726.8898  w_gpu=9.120  b_gpu=2.943\n",
            "epoch  50 w_gpu.grad=-0.025  b_gpu.grad=-0.451 \n",
            "epoch  50  loss=1.2351  w_gpu=5.129  b_gpu=3.200\n",
            "epoch 100 w_gpu.grad=0.034  b_gpu.grad=-0.223 \n",
            "epoch 100  loss=1.0889  w_gpu=5.063  b_gpu=3.626\n",
            "epoch 150 w_gpu.grad=0.017  b_gpu.grad=-0.114 \n",
            "epoch 150  loss=1.0510  w_gpu=5.031  b_gpu=3.844\n",
            "epoch 200 w_gpu.grad=0.009  b_gpu.grad=-0.058 \n",
            "epoch 200  loss=1.0411  w_gpu=5.014  b_gpu=3.955\n",
            "epoch 250 w_gpu.grad=0.004  b_gpu.grad=-0.029 \n",
            "epoch 250  loss=1.0386  w_gpu=5.006  b_gpu=4.011\n",
            "epoch 300 w_gpu.grad=0.002  b_gpu.grad=-0.015 \n",
            "epoch 300  loss=1.0379  w_gpu=5.001  b_gpu=4.040\n",
            "epoch 350 w_gpu.grad=0.001  b_gpu.grad=-0.008 \n",
            "epoch 350  loss=1.0378  w_gpu=4.999  b_gpu=4.054\n",
            "epoch 400 w_gpu.grad=0.001  b_gpu.grad=-0.004 \n",
            "epoch 400  loss=1.0377  w_gpu=4.998  b_gpu=4.062\n",
            "epoch 450 w_gpu.grad=0.000  b_gpu.grad=-0.002 \n",
            "epoch 450  loss=1.0377  w_gpu=4.997  b_gpu=4.066\n",
            "epoch 500 w_gpu.grad=0.000  b_gpu.grad=-0.001 \n",
            "epoch 500  loss=1.0377  w_gpu=4.997  b_gpu=4.068\n",
            "GPU 耗时: 0.1213221549987793 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#定义模型, 用class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearReg(nn.Module):             #我要开一家叫 LinearReg 的公司，并且挂靠在巨头集团 nn.Module 旗下（继承）\n",
        "    def __init__(self):             #公司开张第一天，得先把“营业执照”办下来——这就是初始化仪式。在 __init__ 里，self 是空壳对象，你给它装属性、贴方法，把它“装饰成”真正的实例。\n",
        "        super().__init__()          #先去总部（nn.Module）把营业执照盖章（父类初始化），否则你不能享受集团的“水电优惠”（自动求导、保存、搬家等福利）\n",
        "        self.linear = nn.Linear(1, 1)   # 单特征→单输出。公司正式进货：买一台官方现成的“线性机”（nn.Linear），它能做 y = w·x + b, linear挂在 self 身上当属性用\n",
        "\n",
        "    def forward(self, x):           #定义前向传播路径：数据 x 进入模型后，直接传给 self.linear 这一层（即官方线性模块 nn.Linear(1,1)），得到输出 y = w·x + b，无需手动写 w*x + b\n",
        "        return self.linear(x)       #x这里只是形式变量，和forward里面一致就行\n",
        "\n",
        "my_model = LinearReg().to(device)         #在外面要叫LinearReg, self只有在上面类的方法内部才存在，代表“当前这个实例”。self 是“屋里照镜子”，LinearReg() 是“屋外生小孩”my_model\n",
        "\n",
        "#损失 & 优化器\n",
        "criterion = nn.MSELoss()                    #选损失函数：均方误差 (Mean-Squared-Error)\n",
        "#optimizer = torch.optim.SGD(my_model.parameters(), lr=2.7e-2)     #选优化器：随机梯度下降 (Stochastic Gradient Descent),把模型里所有需要训练的张量（weight、bias）一次性交给优化器\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([       #给不同参数组指定不同 lr\n",
        "    {'params': my_model.linear.weight, 'lr': 1e-2},\n",
        "    {'params': my_model.linear.bias,   'lr': 1.5e-2}\n",
        "], lr=1e-3)          # 默认 lr，对未显式分组的生效\n",
        "\n",
        "\n",
        "#训练 500 epoch（模板化 6 步）\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = my_model(X_gpu)\n",
        "    loss = criterion(outputs, y_gpu)\n",
        "    loss.backward()           #反向传播，计算 loss 对 所有参数（w、b）的梯度，存到对应的 .grad 里。\n",
        "    optimizer.step()          #按梯度和学习率更新一次参数：w ← w − lr·grad_w，b ← b − lr·grad_b\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        w_val = my_model.linear.weight.item()   # 取出标量 w\n",
        "        b_val = my_model.linear.bias.item()     # 取出标量 b\n",
        "        print(f'epoch {epoch:d}  loss={loss.item():.4f}  w={w_val:.3f}  b={b_val:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-R2uSjBuy-L",
        "outputId": "2daa3d11-bda3-4638-914d-ae31aab6d31d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  loss=780.4953  w=4.007  b=0.774\n",
            "epoch 50  loss=2.2802  w=5.293  b=2.059\n",
            "epoch 100  loss=1.6577  w=5.193  b=2.724\n",
            "epoch 150  loss=1.3605  w=5.124  b=3.183\n",
            "epoch 200  loss=1.2185  w=5.076  b=3.500\n",
            "epoch 250  loss=1.1508  w=5.043  b=3.719\n",
            "epoch 300  loss=1.1184  w=5.020  b=3.871\n",
            "epoch 350  loss=1.1029  w=5.004  b=3.976\n",
            "epoch 400  loss=1.0956  w=4.993  b=4.048\n",
            "epoch 450  loss=1.0920  w=4.986  b=4.098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 两个特征的模型，用class\n",
        "# 1. 造数据 -----------------------------------------------------------\n",
        "# 真实模型: y = 3*x1 + 5*x2 + 4\n",
        "true_w1, true_w2, true_b = 3.0, 5.0, 4.0\n",
        "n_samples = 100\n",
        "X_cpu = torch.rand(n_samples, 2) * 10          # 100×2 矩阵\n",
        "y_cpu = true_w1 * X_cpu[:, 0] + true_w2 * X_cpu[:, 1] + true_b\n",
        "y_cpu += torch.randn(n_samples) * 0.5          # 加噪声\n",
        "X2D_gpu = X_cpu.to(device)  #小心，上面多个celll里面是1个特征，这里2个，所以改个名字\n",
        "y2D_gpu = y_cpu.to(device)\n",
        "\n",
        "# 2. 定义模型 ---------------------------------------------------------\n",
        "class LinearReg2D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2, 1)   # 2 个特征 → 1 个输出\n",
        "\n",
        "    def forward(self, x):           #x这里只是形式变量\n",
        "        return self.linear(x)           # y = w1*x1 + w2*x2 + b\n",
        "\n",
        "model = LinearReg2D().to(device)\n",
        "\n",
        "# 3. 损失 & 优化器 -----------------------------------------------------\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1.5e-2)\n",
        "\n",
        "optimizer = torch.optim.SGD([ {'params': model.linear.weight, 'lr': 1.5e-2},\n",
        "    {'params': model.linear.bias,   'lr': 2.5e-2}],\n",
        "    lr=1.5e-2)          # 默认 lr，对未显式分组的生效\n",
        "\n",
        "\n",
        "# 4. 训练 -------------------------------------------------------------\n",
        "for epoch in range(600):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X2D_gpu)              # 前向\n",
        "    loss = criterion(outputs.squeeze(), y2D_gpu)  # squeeze 保证形状一致\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 60 == 0:\n",
        "        w1, w2 = model.linear.weight[0].tolist()  # 取出 w1, w2\n",
        "        b = model.linear.bias.item()              # 取出 b\n",
        "        print(f'epoch {epoch:3d}  loss={loss.item():.4f}  '\n",
        "              f'w1={w1:.3f}  w2={w2:.3f}  b={b:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3naNkDyyrwA",
        "outputId": "a35405ec-7e28-4ac6-8ad2-5155f8022825"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   0  loss=2340.5554  w1=7.944  w2=8.644  b=1.759\n",
            "epoch  60  loss=22.9899  w1=3.546  w2=5.593  b=1.946\n",
            "epoch 120  loss=0.7581  w1=3.143  w2=5.167  b=2.604\n",
            "epoch 180  loss=0.3806  w1=3.074  w2=5.086  b=3.089\n",
            "epoch 240  loss=0.3054  w1=3.048  w2=5.053  b=3.412\n",
            "epoch 300  loss=0.2739  w1=3.033  w2=5.033  b=3.624\n",
            "epoch 360  loss=0.2605  w1=3.023  w2=5.020  b=3.762\n",
            "epoch 420  loss=0.2547  w1=3.016  w2=5.011  b=3.853\n",
            "epoch 480  loss=0.2522  w1=3.012  w2=5.006  b=3.913\n",
            "epoch 540  loss=0.2512  w1=3.009  w2=5.002  b=3.952\n"
          ]
        }
      ]
    }
  ]
}